# GPT-2 - Treinamento do Zero

<img width="1000" height="600" alt="train_an_val_loss" src="https://github.com/user-attachments/assets/eec009b4-d329-4e2d-a5db-83fab7772bda" />


Implementa√ß√£o e treinamento de um modelo GPT-2 com 15 milh√µes de par√¢metros, incluindo an√°lise detalhada de complexidade computacional.

##  √çndice

- [Vis√£o Geral](#vis√£o-geral)
- [Requisitos do Sistema](#requisitos-do-sistema)
- [Instala√ß√£o](#instala√ß√£o)
- [Como Executar](#como-executar)
- [Arquitetura do Modelo](#arquitetura-do-modelo)
- [Configura√ß√£o de Treinamento](#configura√ß√£o-de-treinamento)
- [An√°lise de Complexidade](#an√°lise-de-complexidade)
- [M√©tricas de Performance](#m√©tricas-de-performance)
- [Estrutura do Projeto](#estrutura-do-projeto)

##  Vis√£o Geral

Este projeto implementa uma arquitetura GPT-2 do zero usando PyTorch, focando no entendimento dos requisitos computacionais e desafios de otimiza√ß√£o de modelos de linguagem baseados em transformers. A implementa√ß√£o segue as especifica√ß√µes do paper original do GPT-2 com contagem reduzida de par√¢metros adequada para treinamento em GPU √∫nica.

### Caracter√≠sticas Principais

- **15.103.616 par√¢metros** totais
- **4 camadas** de transformer
- **256 dimens√µes** de embedding
- **4 cabe√ßas** de aten√ß√£o
- **256 tokens** de comprimento de contexto
- **Vocabul√°rio** de 50.304 tokens

## üíª Requisitos do Sistema

### Hardware M√≠nimo

- **GPU**: NVIDIA com pelo menos 6GB VRAM (testado em RTX 4050)
- **RAM**: 16GB recomendado
- **Armazenamento**: 5GB para dataset e checkpoints

### Software

- **Sistema Operacional**: Linux, Windows 10/11, ou macOS
- **Python**: 3.8 ou superior
- **CUDA**: 11.8 ou superior (para GPU NVIDIA)

### Depend√™ncias Python

```txt
torch>=2.0.0
numpy>=1.24.0
tiktoken>=0.5.0
tqdm>=4.65.0
matplotlib>=3.7.0
```

##  Instala√ß√£o

### 1. Clone o Reposit√≥rio

```bash
git clone https://github.com/seu-usuario/gpt2-from-scratch.git
cd gpt2-from-scratch
```

### 2. Crie o Ambiente Virtual

**No Linux/macOS:**
```bash
python3 -m venv venv
source venv/bin/activate
```

**No Windows:**
```cmd
python -m venv venv
venv\Scripts\activate
```

### 3. Instale as Depend√™ncias

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Verifique a Instala√ß√£o do PyTorch com CUDA

```bash
python -c "import torch; print(f'CUDA dispon√≠vel: {torch.cuda.is_available()}')"
```

##  Como Executar

### Ativa√ß√£o do Ambiente Virtual

Antes de executar o script, sempre ative o ambiente virtual:

**Linux/macOS:**
```bash
source venv/bin/activate
```

**Windows:**
```cmd
venv\Scripts\activate
```

### Execu√ß√£o do Treinamento

```bash
python gpt2.py
```

### Op√ß√µes de Linha de Comando

```bash
# Treinamento b√°sico
python gpt2.py

# Com configura√ß√µes customizadas
python gpt2.py --batch-size 64 --learning-rate 1e-4 --epochs 10

# Continuar de um checkpoint
python gpt2.py --resume checkpoint.pt

# Modo de avalia√ß√£o apenas
python gpt2.py --eval-only --checkpoint model_final.pt
```

### Desativa√ß√£o do Ambiente Virtual

Quando terminar:
```bash
deactivate
```

##  Arquitetura do Modelo

### Especifica√ß√µes

| Componente | Especifica√ß√£o |
|-----------|--------------|
| Par√¢metros Totais | 15.103.616 |
| Camadas Transformer | 4 |
| Dimens√£o de Embedding | 256 |
| Cabe√ßas de Aten√ß√£o | 4 |
| Comprimento de Contexto | 256 tokens |
| Tamanho do Vocabul√°rio | 50.304 |
| Taxa de Dropout | 0.7 |
| Fun√ß√£o de Ativa√ß√£o | GELU |

### Componentes da Arquitetura

**Token Embedding**: 50.304 √ó 256 = 12.877.824 par√¢metros

**Positional Embedding**: 256 √ó 256 = 65.536 par√¢metros

**Blocos Transformer** (4 camadas):
- Auto-aten√ß√£o multi-cabe√ßa com mascaramento causal
- Rede feed-forward posicional (expans√£o 4√ó)
- Normaliza√ß√£o de camada
- Conex√µes residuais

## ‚öôÔ∏è Configura√ß√£o de Treinamento

### Hiperpar√¢metros

```python
batch_size = 32
sequence_length = 256
learning_rate = 3e-4  # Otimizador AdamW
weight_decay = 0.1
beta1 = 0.9
beta2 = 0.95
gradient_clip = 1.0
```

### Dataset

- **Conjunto de Treinamento**: 4M tokens
- **Batches por √âpoca**: 2.848
- **Tokens por √âpoca**: 23.330.816

## üìä An√°lise de Complexidade

### Complexidade de Tempo

Para um modelo Transformer com L camadas, dimens√£o d e comprimento de sequ√™ncia n:

**Camada de Auto-Aten√ß√£o**: O(n¬≤ √ó d)
- Proje√ß√µes Query, Key, Value: O(n √ó d¬≤)
- Computa√ß√£o de aten√ß√£o: O(n¬≤ √ó d)
- Proje√ß√£o de sa√≠da: O(n √ó d¬≤)

**Rede Feed-Forward**: O(n √ó d¬≤)
- Duas transforma√ß√µes lineares com dimens√£o intermedi√°ria 4d

**Total por Camada**: O(n¬≤ √ó d + n √ó d¬≤)

**Modelo Completo**: O(L √ó (n¬≤ √ó d + n √ó d¬≤))

### C√°lculo de FLOPs

**Forward Pass**:
```
FLOPs = 6 √ó par√¢metros √ó sequence_length
FLOPs = 6 √ó 15.103.616 √ó 256
FLOPs = 23,2 GFLOPs por sequ√™ncia
```

**Treinamento (Forward + Backward)**:
```
FLOPs = 23,2 √ó 3 = 69,6 GFLOPs por sequ√™ncia
```

**Por √âpoca**:
```
Sequ√™ncias = 23.330.816 / 256 = 91.136
Total FLOPs = 69,6 √ó 10‚Åπ √ó 91.136 = 6,34 √ó 10¬π‚Åµ FLOPs
```

##  M√©tricas de Performance

### Performance de Treinamento

| M√©trica | Valor |
|--------|-------|
| Tokens por √âpoca | 23.330.816 |
| Tempo de Treinamento | 37,5 min/√©poca |
| Throughput | 10.369 tokens/seg |
| Sequ√™ncias/seg | 40,5 |
| Tempo por Batch | 0,79 segundos |
| FLOPs Efetivos | 2,82 TFLOPS |
| MFU (Utiliza√ß√£o de FLOPs) | 4,03% |

### Curvas de Loss

**Loss de Treinamento**:
- Inicial: 13,0
- Final: 5,3
- Redu√ß√£o: 59,2%

**Loss de Valida√ß√£o**:
- Inicial: 13,0
- Final: 9,4
- Estabiliza√ß√£o: ~9,0 ap√≥s 40M tokens

**Gap Treino-Valida√ß√£o**:
- Indica leve overfitting ap√≥s 40M tokens
- Recomendado: early stopping ou aumento de regulariza√ß√£o

##  Estrutura do Projeto

```
gpt2/
‚îú‚îÄ‚îÄ gpt2.py                 # Script principal
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias
‚îú‚îÄ‚îÄ README.md              # Este arquivo
‚îú‚îÄ‚îÄ data/                  # Diret√≥rio de dados
‚îÇ   ‚îú‚îÄ‚îÄ train.txt
‚îÇ   ‚îî‚îÄ‚îÄ val.txt
‚îú‚îÄ‚îÄ checkpoints/           # Checkpoints do modelo
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint_epoch_1.pt
‚îÇ   ‚îî‚îÄ‚îÄ model_final.pt
‚îî‚îÄ‚îÄ logs/                  # Logs de treinamento
    ‚îî‚îÄ‚îÄ training.log
```

##  Solu√ß√£o de Problemas

### CUDA Out of Memory

Se encontrar erros de mem√≥ria:
```python
# Reduza o batch size
batch_size = 16  # ou 8

# Ou reduza o sequence length
sequence_length = 128
```

### Performance Lenta

1. Verifique se CUDA est√° dispon√≠vel
2. Use mixed precision training (FP16)
3. Aumente o batch size se houver VRAM dispon√≠vel
4. Use gradient accumulation para batches maiores efetivos

##  Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.

##  Contribui√ß√µes

Contribui√ß√µes s√£o bem-vindas! Por favor:

1. Fa√ßa fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/NovaFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Adiciona nova feature'`)
4. Push para a branch (`git push origin feature/NovaFeature`)
5. Abra um Pull Request

##  Contato

Para d√∫vidas ou sugest√µes, abra uma issue no GitHub.

- OpenAI pelo paper original do GPT-2
- Comunidade PyTorch
- Andrej Karpathy pelos tutoriais de implementa√ß√£o

---

**Nota**: Este √© um projeto educacional. Para aplica√ß√µes em produ√ß√£o, considere usar implementa√ß√µes otimizadas como Hugging Face Transformers.

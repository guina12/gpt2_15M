# GPT-2 - Treinamento do Zero

ImplementaÃ§Ã£o e treinamento de um modelo GPT-2 com 15 milhÃµes de parÃ¢metros, incluindo anÃ¡lise detalhada de complexidade computacional.

##  Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [Requisitos do Sistema](#requisitos-do-sistema)
- [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
- [Como Executar](#como-executar)
- [Arquitetura do Modelo](#arquitetura-do-modelo)
- [ConfiguraÃ§Ã£o de Treinamento](#configuraÃ§Ã£o-de-treinamento)
- [AnÃ¡lise de Complexidade](#anÃ¡lise-de-complexidade)
- [MÃ©tricas de Performance](#mÃ©tricas-de-performance)
- [Estrutura do Projeto](#estrutura-do-projeto)

##  VisÃ£o Geral

Este projeto implementa uma arquitetura GPT-2 do zero usando PyTorch, focando no entendimento dos requisitos computacionais e desafios de otimizaÃ§Ã£o de modelos de linguagem baseados em transformers. A implementaÃ§Ã£o segue as especificaÃ§Ãµes do paper original do GPT-2 com contagem reduzida de parÃ¢metros adequada para treinamento em GPU Ãºnica.

### CaracterÃ­sticas Principais

- **15.103.616 parÃ¢metros** totais
- **4 camadas** de transformer
- **256 dimensÃµes** de embedding
- **4 cabeÃ§as** de atenÃ§Ã£o
- **256 tokens** de comprimento de contexto
- **VocabulÃ¡rio** de 50.304 tokens

## ğŸ’» Requisitos do Sistema

### Hardware MÃ­nimo

- **GPU**: NVIDIA com pelo menos 6GB VRAM (testado em RTX 4050)
- **RAM**: 16GB recomendado
- **Armazenamento**: 5GB para dataset e checkpoints

### Software

- **Sistema Operacional**: Linux, Windows 10/11, ou macOS
- **Python**: 3.8 ou superior
- **CUDA**: 11.8 ou superior (para GPU NVIDIA)

### DependÃªncias Python

```txt
torch>=2.0.0
numpy>=1.24.0
tiktoken>=0.5.0
tqdm>=4.65.0
matplotlib>=3.7.0
```

## ğŸš€ InstalaÃ§Ã£o

### 1. Clone o RepositÃ³rio

```bash
git clone https://github.com/seu-usuario/gpt2-from-scratch.git
cd gpt2-from-scratch
```

### 2. Crie o Ambiente Virtual

**No Linux/macOS:**
```bash
python3 -m venv venv
source venv/bin/activate
```

**No Windows:**
```cmd
python -m venv venv
venv\Scripts\activate
```

### 3. Instale as DependÃªncias

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Verifique a InstalaÃ§Ã£o do PyTorch com CUDA

```bash
python -c "import torch; print(f'CUDA disponÃ­vel: {torch.cuda.is_available()}')"
```

## â–¶ï¸ Como Executar

### AtivaÃ§Ã£o do Ambiente Virtual

Antes de executar o script, sempre ative o ambiente virtual:

**Linux/macOS:**
```bash
source venv/bin/activate
```

**Windows:**
```cmd
venv\Scripts\activate
```

### ExecuÃ§Ã£o do Treinamento

```bash
python gpt2.py
```

### OpÃ§Ãµes de Linha de Comando

```bash
# Treinamento bÃ¡sico
python gpt2.py

# Com configuraÃ§Ãµes customizadas
python gpt2.py --batch-size 64 --learning-rate 1e-4 --epochs 10

# Continuar de um checkpoint
python gpt2.py --resume checkpoint.pt

# Modo de avaliaÃ§Ã£o apenas
python gpt2.py --eval-only --checkpoint model_final.pt
```

### DesativaÃ§Ã£o do Ambiente Virtual

Quando terminar:
```bash
deactivate
```

## ğŸ—ï¸ Arquitetura do Modelo

### EspecificaÃ§Ãµes

| Componente | EspecificaÃ§Ã£o |
|-----------|--------------|
| ParÃ¢metros Totais | 15.103.616 |
| Camadas Transformer | 4 |
| DimensÃ£o de Embedding | 256 |
| CabeÃ§as de AtenÃ§Ã£o | 4 |
| Comprimento de Contexto | 256 tokens |
| Tamanho do VocabulÃ¡rio | 50.304 |
| Taxa de Dropout | 0.7 |
| FunÃ§Ã£o de AtivaÃ§Ã£o | GELU |

### Componentes da Arquitetura

**Token Embedding**: 50.304 Ã— 256 = 12.877.824 parÃ¢metros

**Positional Embedding**: 256 Ã— 256 = 65.536 parÃ¢metros

**Blocos Transformer** (4 camadas):
- Auto-atenÃ§Ã£o multi-cabeÃ§a com mascaramento causal
- Rede feed-forward posicional (expansÃ£o 4Ã—)
- NormalizaÃ§Ã£o de camada
- ConexÃµes residuais

## âš™ï¸ ConfiguraÃ§Ã£o de Treinamento

### HiperparÃ¢metros

```python
batch_size = 32
sequence_length = 256
learning_rate = 3e-4  # Otimizador AdamW
weight_decay = 0.1
beta1 = 0.9
beta2 = 0.95
gradient_clip = 1.0
```

### Dataset

- **Conjunto de Treinamento**: 4M tokens
- **Batches por Ã‰poca**: 2.848
- **Tokens por Ã‰poca**: 23.330.816

## ğŸ“Š AnÃ¡lise de Complexidade

### Complexidade de Tempo

Para um modelo Transformer com L camadas, dimensÃ£o d e comprimento de sequÃªncia n:

**Camada de Auto-AtenÃ§Ã£o**: O(nÂ² Ã— d)
- ProjeÃ§Ãµes Query, Key, Value: O(n Ã— dÂ²)
- ComputaÃ§Ã£o de atenÃ§Ã£o: O(nÂ² Ã— d)
- ProjeÃ§Ã£o de saÃ­da: O(n Ã— dÂ²)

**Rede Feed-Forward**: O(n Ã— dÂ²)
- Duas transformaÃ§Ãµes lineares com dimensÃ£o intermediÃ¡ria 4d

**Total por Camada**: O(nÂ² Ã— d + n Ã— dÂ²)

**Modelo Completo**: O(L Ã— (nÂ² Ã— d + n Ã— dÂ²))

### CÃ¡lculo de FLOPs

**Forward Pass**:
```
FLOPs = 6 Ã— parÃ¢metros Ã— sequence_length
FLOPs = 6 Ã— 15.103.616 Ã— 256
FLOPs = 23,2 GFLOPs por sequÃªncia
```

**Treinamento (Forward + Backward)**:
```
FLOPs = 23,2 Ã— 3 = 69,6 GFLOPs por sequÃªncia
```

**Por Ã‰poca**:
```
SequÃªncias = 23.330.816 / 256 = 91.136
Total FLOPs = 69,6 Ã— 10â¹ Ã— 91.136 = 6,34 Ã— 10Â¹âµ FLOPs
```

## ğŸ“ˆ MÃ©tricas de Performance

### Performance de Treinamento

| MÃ©trica | Valor |
|--------|-------|
| Tokens por Ã‰poca | 23.330.816 |
| Tempo de Treinamento | 37,5 min/Ã©poca |
| Throughput | 10.369 tokens/seg |
| SequÃªncias/seg | 40,5 |
| Tempo por Batch | 0,79 segundos |
| FLOPs Efetivos | 2,82 TFLOPS |
| MFU (UtilizaÃ§Ã£o de FLOPs) | 4,03% |

### Curvas de Loss

**Loss de Treinamento**:
- Inicial: 13,0
- Final: 5,3
- ReduÃ§Ã£o: 59,2%

**Loss de ValidaÃ§Ã£o**:
- Inicial: 13,0
- Final: 9,4
- EstabilizaÃ§Ã£o: ~9,0 apÃ³s 40M tokens

**Gap Treino-ValidaÃ§Ã£o**:
- Indica leve overfitting apÃ³s 40M tokens
- Recomendado: early stopping ou aumento de regularizaÃ§Ã£o

##  Estrutura do Projeto

```
gpt2/
â”œâ”€â”€ gpt2.py                 # Script principal
â”œâ”€â”€ requirements.txt        # DependÃªncias
â”œâ”€â”€ README.md              # Este arquivo
â”œâ”€â”€ data/                  # DiretÃ³rio de dados
â”‚   â”œâ”€â”€ train.txt
â”‚   â””â”€â”€ val.txt
â”œâ”€â”€ checkpoints/           # Checkpoints do modelo
â”‚   â”œâ”€â”€ checkpoint_epoch_1.pt
â”‚   â””â”€â”€ model_final.pt
â””â”€â”€ logs/                  # Logs de treinamento
    â””â”€â”€ training.log
```

## ğŸ› SoluÃ§Ã£o de Problemas

### CUDA Out of Memory

Se encontrar erros de memÃ³ria:
```python
# Reduza o batch size
batch_size = 16  # ou 8

# Ou reduza o sequence length
sequence_length = 128
```

### Performance Lenta

1. Verifique se CUDA estÃ¡ disponÃ­vel
2. Use mixed precision training (FP16)
3. Aumente o batch size se houver VRAM disponÃ­vel
4. Use gradient accumulation para batches maiores efetivos

##  LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

##  ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. FaÃ§a fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/NovaFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona nova feature'`)
4. Push para a branch (`git push origin feature/NovaFeature`)
5. Abra um Pull Request

## ğŸ“§ Contato

Para dÃºvidas ou sugestÃµes, abra uma issue no GitHub.

- OpenAI pelo paper original do GPT-2
- Comunidade PyTorch
- Andrej Karpathy pelos tutoriais de implementaÃ§Ã£o

---

**Nota**: Este Ã© um projeto educacional. Para aplicaÃ§Ãµes em produÃ§Ã£o, considere usar implementaÃ§Ãµes otimizadas como Hugging Face Transformers.
